{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aowpvQ64-9fY"
      },
      "source": [
        "# Welcome to Pipelines!\n",
        "\n",
        "The HuggingFace transformers library provides APIs at two different levels.\n",
        "\n",
        "The High Level API for using open-source models for typical inference tasks is called \"pipelines\". It's incredibly easy to use.\n",
        "\n",
        "You create a pipeline using something like:\n",
        "\n",
        "`my_pipeline = pipeline(\"the_task_I_want_to_do\")`\n",
        "\n",
        "Followed by\n",
        "\n",
        "`result = my_pipeline(my_input)`\n",
        "\n",
        "And that's it!\n",
        "\n",
        "See end of this colab for a list of all pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dna1Uth1--Z3"
      },
      "source": [
        "## A sidenote:\n",
        "\n",
        "You may already know this, but just in case you're not familiar with the word \"inference\" that I use here:\n",
        "\n",
        "When working with Data Science models, you could be carrying out 2 very different activities: **training** and **inference**.\n",
        "\n",
        "### 1. Training  \n",
        "\n",
        "**Training** is when you provide a model with data for it to adapt to get better at a task in the future. It does this by updating its internal settings - the parameters or weights of the model. If you're Training a model that's already had some training, the activity is called \"fine-tuning\".\n",
        "\n",
        "### 2. Inference\n",
        "\n",
        "**Inference** is when you are working with a model that has _already been trained_. You are using that model to produce new outputs on new inputs, taking advantage of everything it learned while it was being trained. Inference is also sometimes referred to as \"Execution\" or \"Running a model\".\n",
        "\n",
        "All of our use of APIs for GPT, Claude and Gemini in the last weeks are examples of **inference**. The \"P\" in GPT stands for \"Pre-trained\", meaning that it has already been trained with data (lots of it!) In week 6 we will try fine-tuning GPT ourselves.\n",
        "  \n",
        "The pipelines API in HuggingFace is only for use for **inference** - running a model that has already been trained. In week 7 we will be training our own model, and we will need to use the more advanced HuggingFace APIs that we look at in the up-coming lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47172d72"
      },
      "source": [
        "Executes the code snippet: `!pip install -q transformers datasets diffusers...`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkHbcf_7_GhK"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets diffusers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b01ce87"
      },
      "source": [
        "Imports the following modules and packages: IPython.display, datasets, diffusers, google.colab, huggingface_hub, soundfile, torch, transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPiL7hYU_HW_"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline\n",
        "from diffusers import DiffusionPipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7aa97d2"
      },
      "source": [
        "Executes the code snippet: `hf_token = userdata.get('HF_TOKEN')...`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2nc02F3_NwW"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c247393"
      },
      "source": [
        "Executes the code snippet: `print(torch.cuda.is_available())...`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V9WOXey_tR7"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "108d2a5c"
      },
      "source": [
        "Initializes Hugging Face pipeline(s) for: sentiment-analysis tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZvr-bwe_Trf"
      },
      "outputs": [],
      "source": [
        "#Sentiment Analysis\n",
        "classifier = pipeline(\"sentiment-analysis\", device=\"cuda\")\n",
        "result = classifier(\"I'm super excited to be on the way to LLM mastery!\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b992ecb8"
      },
      "source": [
        "Initializes Hugging Face pipeline(s) for: Named Entity Recognition tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAS-VRhk_XRX"
      },
      "outputs": [],
      "source": [
        "#Named Entity Recognition\n",
        "ner = pipeline(\"ner\", grouped_entities=True, device=\"cuda\")\n",
        "result = ner(\"Barack Obama was the 44th president of the United States.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c202a246"
      },
      "source": [
        "Initializes Hugging Face pipeline(s) for: question-answering tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsJO4RxZAPLh"
      },
      "outputs": [],
      "source": [
        "#Question Answering with Context\n",
        "question_answerer = pipeline(\"question-answering\", device=\"cuda\")\n",
        "result = question_answerer(question=\"Who was the 44th president of the United States?\", context=\"Barack Obama was the 44th president of the United States.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f0d583c"
      },
      "source": [
        "Initializes Hugging Face pipeline(s) for: summarization tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMbnmhlyAXXD"
      },
      "outputs": [],
      "source": [
        "#Text Summarization\n",
        "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
        "text = \"\"\"The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
        "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
        "It's an extremely popular library that's widely used by the open-source data science community.\n",
        "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f45330b"
      },
      "source": [
        "Initializes Hugging Face pipeline(s) for: translation_en_to_fr tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCVa0OLhAjj_"
      },
      "outputs": [],
      "source": [
        "#Translation\n",
        "translator = pipeline(\"translation_en_to_fr\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a3751af"
      },
      "source": [
        "Initializes Hugging Face pipeline(s) for: translation_en_to_es tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJrRU3kCCTwT"
      },
      "outputs": [],
      "source": [
        "# Another translation, showing a model being specified\n",
        "# All translation models are here: https://huggingface.co/models?pipeline_tag=translation&sort=trending\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a107756d"
      },
      "source": [
        "Initializes Hugging Face pipeline(s) for: zero-shot-classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYwB60qXCaMv"
      },
      "outputs": [],
      "source": [
        "#Classification\n",
        "classifier = pipeline(\"zero-shot-classification\", device=\"cuda\")\n",
        "result = classifier(\"Hugging Face's Transformers library is amazing!\", candidate_labels=[\"technology\", \"sports\", \"politics\"])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "867c0008"
      },
      "source": [
        "Initializes Hugging Face pipeline(s) for: text-generation tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8E4NiykCijr"
      },
      "outputs": [],
      "source": [
        "#Text Generation\n",
        "generator = pipeline(\"text-generation\", device=\"cuda\")\n",
        "result = generator(\"If there's one thing I want you to remember about using HuggingFace pipelines, it's\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c190e668"
      },
      "source": [
        "Loads pretrained model(s): stabilityai/stable-diffusion-2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDF8hqS0Cuv-"
      },
      "outputs": [],
      "source": [
        "#Image Generation\n",
        "image_gen = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "text = \"A Lamborghini Urus Car Drifting in a race course\"\n",
        "image = image_gen(prompt=text).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfb7178e"
      },
      "source": [
        "Initializes Hugging Face pipeline(s) for: text-to-speech tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciMfqNbHC6K3"
      },
      "outputs": [],
      "source": [
        "#Audio Generation\n",
        "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cuda')\n",
        "\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "speech = synthesiser(\"Hi, My Name is Rishi Kora graduated from The University of Essex\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "\n",
        "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
        "Audio(\"speech.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64d51208"
      },
      "source": [
        "Imports the following modules and packages: diffusers, scipy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46WMYnVqGpjQ"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "from diffusers import AudioLDM2Pipeline\n",
        "\n",
        "repo_id = \"cvssp/audioldm2\"\n",
        "pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# define the prompts\n",
        "prompt = \"English Rap about love\"\n",
        "negative_prompt = \"High quality.\"\n",
        "\n",
        "# set the seed for generator\n",
        "generator = torch.Generator(\"cuda\").manual_seed(0)\n",
        "\n",
        "# run the generation\n",
        "audio = pipe(\n",
        "    prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    num_inference_steps=200,\n",
        "    audio_length_in_s=10.0,\n",
        "    num_waveforms_per_prompt=3,\n",
        "    generator=generator,\n",
        ").audios\n",
        "\n",
        "# save the best audio sample (index 0) as a .wav file\n",
        "scipy.io.wavfile.write(\"Lover.wav\", rate=16000, data=audio[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}